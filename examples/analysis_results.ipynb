{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59dbb13e",
            "metadata": {},
            "outputs": [],
            "source": [
                "{\n",
                "    \"cells\": [\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\"# PoC 2: EU Contract Clause Extraction - Results Analysis\"],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\n",
                "                \"## 1. Setup and Configuration\\n\",\n",
                "                \"\\n\",\n",
                "                \"This notebook analyzes the evaluation results from various RAG configurations and a baseline model. It loads data from `.jsonl` files, processes metrics, and presents summary tables and visualizations.\",\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                \"import pandas as pd\\n\",\n",
                "                \"import json\\n\",\n",
                "                \"import os\\n\",\n",
                "                \"import numpy as np\\n\",\n",
                "                \"from typing import Any, Dict, List\\n\",\n",
                "                \"import matplotlib.pyplot as plt\\n\",\n",
                "                \"import seaborn as sns\\n\",\n",
                "                \"\\n\",\n",
                "                \"# Ensure plots are displayed inline in Jupyter\\n\",\n",
                "                \"%matplotlib inline\\n\",\n",
                "                'sns.set_theme(style=\"whitegrid\")\\n',\n",
                "                \"\\n\",\n",
                "                \"# --- Configuration ---\\n\",\n",
                "                \"# Assuming the notebook is in `eucontract-analyzer/notebooks/`\\n\",\n",
                "                \"# RESULTS_DIR should point to the `eucontract-analyzer` directory where `rag_evaluation_results` etc. are located\\n\",\n",
                "                'RESULTS_DIR = \"..\" \\n',\n",
                "                \"\\n\",\n",
                "                \"CONFIGS = [\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"Baseline (Claude 3.5, No Judge)\",\\n',\n",
                "                '        r\"baseline_evaluation_results/baseline_eval_summary_eu-clauses-gold-en-v1_claude-3-5_20250517_225906.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (GPT-4.1)\", # Assuming K=5 or not specified, no judge\\n',\n",
                "                '        r\"rag_evaluation_results/rag_eval_summary_eu-clauses-gold-en-v1_gpt-4.1_20250515_125739.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    # --- GPT-4.1-mini RAG ---\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (GPT-4.1-mini, K=3, No Judge)\",\\n',\n",
                "                '        r\"rag_evaluation_results/rag_eval_summary_eu-clauses-gold-en-v1_gpt-4.1-mini_20250516_015403.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (GPT-4.1-mini, K=5, No Judge)\",\\n',\n",
                "                '        r\"rag_evaluation_results/rag_eval_k5_gpt-4.1-mini_eu-clauses-gold-en-v1_20250520_214813.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (GPT-4.1-mini, K=5, Judge: Claude 3.5)\",\\n',\n",
                "                '        r\"rag_evaluation_results/ragevalK5__J-claude-3-5JUDGE_gpt-4.1-mini_text-embedding-3-small_eu-clauses-gold-en-v1_20250522_014404.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (GPT-4.1-mini, K=5, Judge: GPT-4.1-mini)\",\\n',\n",
                "                '        r\"rag_evaluation_results/ragevalK5__J-gpt-4.1-miniJUDGE_gpt-4.1-mini_text-embedding-3-small_eu-clauses-gold-en-v1_20250522_021620.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (GPT-4.1-mini, K=5, Judge: Gemma3)\",\\n',\n",
                "                '        r\"rag_evaluation_results/ragevalK5__J-gemma3-4b-it-qatJUDGE_gpt-4.1-mini_text-embedding-3-small_eu-clauses-gold-en-v1_20250522_023100.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    # --- Gemma-3 RAG ---\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (Gemma-3, K=3, No Judge)\",\\n',\n",
                "                '        r\"rag_evaluation_results/rag_eval_summary_gemma3-4b-it-qat_eu-clauses-gold-en-v1_20250520_030618.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (Gemma-3, K=5, No Judge)\",\\n',\n",
                "                '        r\"rag_evaluation_results/rag_eval_k5_gemma3-4b-it-qat_eu-clauses-gold-en-v1_20250520_164508.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (Gemma-3, K=5, Judge: Gemma3)\",\\n',\n",
                "                '        r\"rag_evaluation_results/ragevalK5__J-gemma3-4b-it-qatJUDGE_gemma3-4b-it-qat_text-embedding-3-small_eu-clauses-gold-en-v1_20250522_034110.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    # --- Claude 3.7 RAG ---\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (Claude 3.7, K=3, No Judge)\",\\n',\n",
                "                '        r\"rag_evaluation_results/rag_eval_summary_claude-3-7_eu-clauses-gold-en-v1_20250520_162138.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"    (\\n\",\n",
                "                '        \"RAG (Claude 3.7, K=5, No Judge)\",\\n',\n",
                "                '        r\"rag_evaluation_results/rag_eval_k5_claude-3-7_eu-clauses-gold-en-v1_20250520_163412.jsonl\",\\n',\n",
                "                \"    ),\\n\",\n",
                "                \"]\",\n",
                "            ],\n",
                "        },\n",
                "        {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"## 2. Helper Functions\"]},\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                \"def load_jsonl(fn: str) -> pd.DataFrame:\\n\",\n",
                "                \"    path = os.path.join(RESULTS_DIR, fn)\\n\",\n",
                "                '    print(f\"Loading → {path}\")\\n',\n",
                "                \"    try:\\n\",\n",
                "                '        with open(path, \"r\", encoding=\"utf-8\") as f:\\n',\n",
                "                \"            return pd.DataFrame([json.loads(l) for l in f])\\n\",\n",
                "                \"    except FileNotFoundError:\\n\",\n",
                "                '        print(f\"  ⚠️ FileNotFoundError: {path}. Returning empty DataFrame.\")\\n',\n",
                "                \"        return pd.DataFrame()\\n\",\n",
                "                \"    except Exception as e:\\n\",\n",
                "                '        print(f\"  ⚠️ {e.__class__.__name__} loading {path}. Returning empty DataFrame.\")\\n',\n",
                "                \"        return pd.DataFrame()\\n\",\n",
                "                \"\\n\",\n",
                "                \"def flatten(df: pd.DataFrame) -> pd.DataFrame:\\n\",\n",
                "                '    if df.empty or \"metrics\" not in df.columns:\\n',\n",
                "                \"        return df\\n\",\n",
                "                \"    # Ensure 'metrics' column exists and handle potential non-dict entries gracefully\\n\",\n",
                "                '    m = df.pop(\"metrics\").apply(lambda d: d if isinstance(d, dict) else {})\\n',\n",
                "                \"    \\n\",\n",
                "                \"    # Define keys to extract from the metrics dictionary\\n\",\n",
                "                \"    metric_keys = [\\n\",\n",
                "                '        \"type_set_precision\",\\n',\n",
                "                '        \"type_set_recall\",\\n',\n",
                "                '        \"type_set_f1\",\\n',\n",
                "                '        \"retrieval_recall\",\\n',\n",
                "                \"    ]\\n\",\n",
                "                \"    for k in metric_keys:\\n\",\n",
                "                \"        # Use pd.NA for missing values, which is Pandas' preferred missing value indicator\\n\",\n",
                "                \"        df[k] = m.apply(lambda d: d.get(k, pd.NA))\\n\",\n",
                "                \"    return df\\n\",\n",
                "                \"\\n\",\n",
                "                \"def summarize(df: pd.DataFrame, label: str) -> Dict[str, Any]:\\n\",\n",
                "                \"    if df.empty:\\n\",\n",
                "                    return {\\n",
                "                        \"Model\": label, \"F1\": np.nan, \"Prec\": np.nan, \"Rec\": np.nan,\\n",
                "                        \"RetRec\": np.nan, \"JudgeScore\": np.nan, \"Succ\": \"0/0\",\\n",
                "                    }\\n",
                "                \"\\n\",\n",
                "                \"    raw_metric_cols = {\\n\",\n",
                "                '        \"F1\": \"type_set_f1\",\\n',\n",
                "                '        \"Prec\": \"type_set_precision\",\\n',\n",
                "                '        \"Rec\": \"type_set_recall\",\\n',\n",
                "                '        \"RetRec\": \"retrieval_recall\",\\n',\n",
                "                '        \"JudgeScore\": \"llm_judge_overall_quality_score\",\\n',\n",
                "                \"    }\\n\",\n",
                "                \"    processed_metrics = {}\\n\",\n",
                "                \"    for display_key, raw_key in raw_metric_cols.items():\\n\",\n",
                "                \"        if raw_key in df.columns:\\n\",\n",
                "                \"            numeric_series = pd.to_numeric(df[raw_key], errors='coerce')\\n\",\n",
                "                \"            processed_metrics[display_key] = numeric_series.mean()\\n\",\n",
                "                \"        else:\\n\",\n",
                "                \"            processed_metrics[display_key] = np.nan\\n\",\n",
                "                \"\\n\",\n",
                "                '    succ = df[\"error\"].isna().sum() if \"error\" in df.columns else len(df)\\n',\n",
                "                \"    total = len(df)\\n\",\n",
                "                \"\\n\",\n",
                "                \"    return {\\n\",\n",
                "                '        \"Model\": label,\\n',\n",
                "                '        \"F1\": processed_metrics.get(\"F1\", np.nan),\\n',\n",
                "                '        \"Prec\": processed_metrics.get(\"Prec\", np.nan),\\n',\n",
                "                '        \"Rec\": processed_metrics.get(\"Rec\", np.nan),\\n',\n",
                "                '        \"RetRec\": processed_metrics.get(\"RetRec\", np.nan),\\n',\n",
                "                '        \"JudgeScore\": processed_metrics.get(\"JudgeScore\", np.nan),\\n',\n",
                "                '        \"Succ\": f\"{succ}/{total}\",\\n',\n",
                "                \"    }\\n\",\n",
                "                \"\\n\",\n",
                "                \"metric_key_map = {\\n\",\n",
                "                '    \"F1\": \"F1\", \"Prec\": \"P\", \"Rec\": \"R\", \"RetRec\": \"RR\", \"JudgeScore\": \"JudgeS\"\\n',\n",
                "                \"}\\n\",\n",
                "                \"\\n\",\n",
                "                \"def extract_delta(summary_df: pd.DataFrame, base_model_name: str) -> Dict[str, Any]:\\n\",\n",
                "                \"    k3_row = pd.Series(dtype=object)\\n\",\n",
                "                \"    k5_row = pd.Series(dtype=object)\\n\",\n",
                "                \"\\n\",\n",
                "                '    k3_candidates = summary_df[summary_df[\"Model\"].str.contains(f\"{base_model_name}, K=3\")]\\n',\n",
                "                '    k5_candidates = summary_df[summary_df[\"Model\"].str.contains(f\"{base_model_name}, K=5\")]\\n',\n",
                "                \"\\n\",\n",
                "                \"    if not k3_candidates.empty:\\n\",\n",
                "                \"        k3_row = k3_candidates.iloc[-1] # Pick last one, assumes judged version if multiple\\n\",\n",
                "                \"    if not k5_candidates.empty:\\n\",\n",
                "                \"        k5_row = k5_candidates.iloc[-1]\\n\",\n",
                "                \"\\n\",\n",
                "                \"    if k3_row.empty or k5_row.empty:\\n\",\n",
                "                '        print(f\"Warning: Missing K=3 or K=5 data for model base {base_model_name}. Skipping delta.\")\\n',\n",
                "                \"        return {\\n\",\n",
                "                '            \"Model\": base_model_name, \"F1@3\": np.nan, \"F1@5\": np.nan, \"ΔF1\": np.nan,\\n',\n",
                "                '            \"P@3\": np.nan, \"P@5\": np.nan, \"ΔP\": np.nan, \"R@3\": np.nan, \"R@5\": np.nan, \"ΔR\": np.nan,\\n',\n",
                "                '            \"RR@3\": np.nan, \"RR@5\": np.nan, \"ΔRR\": np.nan, \"JudgeS@3\": np.nan, \"JudgeS@5\": np.nan,\\n',\n",
                "                '            \"Succ@3\": \"N/A\", \"Succ@5\": \"N/A\",\\n',\n",
                "                \"        }\\n\",\n",
                "                \"\\n\",\n",
                "                '    result_dict = {\"Model\": base_model_name}\\n',\n",
                "                '    metrics_to_compare = [\"F1\", \"Prec\", \"Rec\", \"RetRec\", \"JudgeScore\"]\\n',\n",
                "                \"\\n\",\n",
                "                \"    for metric_key_base in metrics_to_compare:\\n\",\n",
                "                \"        short_metric_key = metric_key_map.get(metric_key_base, metric_key_base)\\n\",\n",
                "                \"        val_k3 = k3_row.get(metric_key_base, np.nan)\\n\",\n",
                "                \"        val_k5 = k5_row.get(metric_key_base, np.nan)\\n\",\n",
                "                \"        \\n\",\n",
                "                \"        val_k3 = float(val_k3) if pd.notna(val_k3) else np.nan\\n\",\n",
                "                \"        val_k5 = float(val_k5) if pd.notna(val_k5) else np.nan\\n\",\n",
                "                \"\\n\",\n",
                "                '        result_dict[f\"{short_metric_key}@3\"] = val_k3\\n',\n",
                "                '        result_dict[f\"{short_metric_key}@5\"] = val_k5\\n',\n",
                "                \"\\n\",\n",
                "                '        if metric_key_base != \"JudgeScore\":\\n',\n",
                "                \"            if pd.notna(val_k3) and pd.notna(val_k5):\\n\",\n",
                "                '                result_dict[f\"Δ{short_metric_key}\"] = val_k5 - val_k3\\n',\n",
                "                \"            else:\\n\",\n",
                "                '                result_dict[f\"Δ{short_metric_key}\"] = np.nan\\n',\n",
                "                \"    \\n\",\n",
                "                '    result_dict[\"Succ@3\"] = k3_row.get(\"Succ\", \"N/A\")\\n',\n",
                "                '    result_dict[\"Succ@5\"] = k5_row.get(\"Succ\", \"N/A\")\\n',\n",
                "                \"    return result_dict\",\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\n",
                "                \"## 3. Load and Process Data\\n\",\n",
                "                \"\\n\",\n",
                "                \"Load all configured result files, flatten their metrics, and store them.\",\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                \"frames: Dict[str, pd.DataFrame] = {}\\n\",\n",
                "                \"for label, fn in CONFIGS:\\n\",\n",
                "                \"    df = load_jsonl(fn)\\n\",\n",
                "                \"    df = flatten(df) # Flatten also handles pd.NA for RAG specific metrics\\n\",\n",
                "                \"    frames[label] = df\\n\",\n",
                "                \"    \\n\",\n",
                "                \"    # Optional: Display head of each loaded DataFrame for quick inspection\\n\",\n",
                "                '    # print(f\"\\\\n--- Head of: {label} ---\")\\n',\n",
                "                \"    # if not df.empty:\\n\",\n",
                "                \"    #     display(df[['celex_id', 'type_set_f1', 'llm_judge_overall_quality_score', 'error']].head() if 'llm_judge_overall_quality_score' in df.columns \\n\",\n",
                "                \"    #               else df[['celex_id', 'type_set_f1', 'error']].head() if 'type_set_f1' in df.columns else df.head())\\n\",\n",
                "                \"    # else:\\n\",\n",
                "                '    #     print(\"  (empty)\")',\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\"## 4. Main Performance Comparison\"],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                \"summary_list = []\\n\",\n",
                "                \"for label, _ in CONFIGS:\\n\",\n",
                "                \"    summary_list.append(summarize(frames[label], label))\\n\",\n",
                "                \"summary_df = pd.DataFrame(summary_list)\\n\",\n",
                "                \"\\n\",\n",
                "                \"# Explicitly ensure numeric columns are float, coercing errors to NaN\\n\",\n",
                "                'final_numeric_cols = [\"F1\", \"Prec\", \"Rec\", \"RetRec\", \"JudgeScore\"]\\n',\n",
                "                \"for col in final_numeric_cols:\\n\",\n",
                "                \"    if col in summary_df.columns:\\n\",\n",
                "                \"        summary_df[col] = pd.to_numeric(summary_df[col], errors='coerce')\\n\",\n",
                "                \"    else:\\n\",\n",
                "                \"        summary_df[col] = np.nan\\n\",\n",
                "                \"\\n\",\n",
                "                'print(\"--- Main Performance Comparison (including LLM Judge Scores) ---\")\\n',\n",
                "                'display(summary_df.style.format(\"{:.3f}\", subset=pd.IndexSlice[:, final_numeric_cols], na_rep=\"N/A\"))',\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\"### 4.1. Visualizing Main Performance Metrics\"],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                \"# Plotting F1 Scores\\n\",\n",
                "                \"plt.figure(figsize=(12, 8))\\n\",\n",
                "                'sns.barplot(data=summary_df.sort_values(\"F1\", ascending=False), y=\"Model\", x=\"F1\", palette=\"viridis\")\\n',\n",
                "                \"plt.title('Model Comparison: Average F1 Score')\\n\",\n",
                "                \"plt.xlabel('Average F1 Score')\\n\",\n",
                "                \"plt.ylabel('Model Configuration')\\n\",\n",
                "                \"plt.tight_layout()\\n\",\n",
                "                \"plt.show()\\n\",\n",
                "                \"\\n\",\n",
                "                \"# Plotting Judge Scores (for models that have them)\\n\",\n",
                "                'judge_score_df = summary_df.dropna(subset=[\"JudgeScore\"])\\n',\n",
                "                \"if not judge_score_df.empty:\\n\",\n",
                "                \"    plt.figure(figsize=(12, max(6, len(judge_score_df) * 0.5)))\\n\",\n",
                "                '    sns.barplot(data=judge_score_df.sort_values(\"JudgeScore\", ascending=False), y=\"Model\", x=\"JudgeScore\", palette=\"mako\")\\n',\n",
                "                \"    plt.title('Model Comparison: Average LLM Judge Score')\\n\",\n",
                "                \"    plt.xlabel('Average Judge Score (0.0-1.0)')\\n\",\n",
                "                \"    plt.ylabel('Model Configuration')\\n\",\n",
                "                \"    plt.tight_layout()\\n\",\n",
                "                \"    plt.show()\\n\",\n",
                "                \"else:\\n\",\n",
                "                '    print(\"No models with Judge Scores to plot.\")',\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\"## 5. K=3 vs. K=5 Comparison\"],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                'bases = [\"GPT-4.1-mini\", \"Gemma-3\", \"Claude 3.7\"]\\n',\n",
                "                \"cmp_rows = [extract_delta(summary_df, b) for b in bases]\\n\",\n",
                "                \"k_comparison_df = pd.DataFrame(cmp_rows)\\n\",\n",
                "                \"\\n\",\n",
                "                \"# Drop rows if all key metric values are NA (helps if a base model was not found for K3/K5)\\n\",\n",
                "                \"metric_value_cols = [col for col in k_comparison_df.columns if col not in ['Model', 'Succ@3', 'Succ@5']]\\n\",\n",
                "                \"k_comparison_df.dropna(subset=metric_value_cols, how='all', inplace=True)\\n\",\n",
                "                \"\\n\",\n",
                "                'print(\"--- K=3 vs K=5 Quant Comparison (including LLM Judge Scores) ---\")\\n',\n",
                "                \"if not k_comparison_df.empty:\\n\",\n",
                "                \"    # Identify float columns for formatting, excluding 'Model', 'Succ@3', 'Succ@5'\\n\",\n",
                "                \"    float_cols_k_comp = [c for c in k_comparison_df.columns if c not in ['Model', 'Succ@3', 'Succ@5']]\\n\",\n",
                "                '    display(k_comparison_df.style.format(\"{:.3f}\", subset=pd.IndexSlice[:, float_cols_k_comp], na_rep=\"N/A\"))\\n',\n",
                "                \"else:\\n\",\n",
                "                '    print(\"  (No data for K=3 vs K=5 comparison table)\")',\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\"### 5.1. Visualizing K=3 vs. K=5 Differences (ΔF1)\"],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                \"if not k_comparison_df.empty and 'ΔF1' in k_comparison_df.columns:\\n\",\n",
                "                '    delta_f1_df = k_comparison_df.dropna(subset=[\"ΔF1\"])\\n',\n",
                "                \"    if not delta_f1_df.empty:\\n\",\n",
                "                \"        plt.figure(figsize=(10, 6))\\n\",\n",
                "                '        sns.barplot(data=delta_f1_df.sort_values(\"ΔF1\", ascending=False), y=\"Model\", x=\"ΔF1\", palette=\"coolwarm\")\\n',\n",
                "                \"        plt.title('Change in F1 Score (K=5 vs. K=3)')\\n\",\n",
                "                \"        plt.xlabel('ΔF1 (F1@5 - F1@3)')\\n\",\n",
                "                \"        plt.ylabel('Base Model')\\n\",\n",
                "                \"        plt.axvline(0, color='grey', linestyle='--') # Add a line at zero for reference\\n\",\n",
                "                \"        plt.tight_layout()\\n\",\n",
                "                \"        plt.show()\\n\",\n",
                "                \"    else:\\n\",\n",
                "                '        print(\"No models with ΔF1 data to plot.\")\\n',\n",
                "                \"else:\\n\",\n",
                "                \"    print(\\\"K comparison DataFrame is empty or missing 'ΔF1' column.\\\")\",\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\n",
                "                \"## 6. Per-Document Analysis (Qualitative Insights)\\n\",\n",
                "                \"\\n\",\n",
                "                \"The `frames` dictionary holds the detailed per-document DataFrames for each configuration. You can use this for deeper qualitative analysis.\\n\",\n",
                "                \"\\n\",\n",
                "                \"For example, to inspect documents where a specific model (e.g., 'RAG (GPT-4.1-mini, K=5, Judge: Claude 3.5)') had a low judge score or a specific error:\",\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"code\",\n",
                "            \"execution_count\": null,\n",
                "            \"metadata\": {},\n",
                "            \"outputs\": [],\n",
                "            \"source\": [\n",
                "                \"model_to_inspect_label = 'RAG (GPT-4.1-mini, K=5, Judge: Claude 3.5)'\\n\",\n",
                "                \"if model_to_inspect_label in frames:\\n\",\n",
                "                \"    df_inspect = frames[model_to_inspect_label]\\n\",\n",
                "                \"    if not df_inspect.empty and 'llm_judge_overall_quality_score' in df_inspect.columns:\\n\",\n",
                "                \"        # Ensure the judge score column is numeric for comparison\\n\",\n",
                "                \"        df_inspect['llm_judge_overall_quality_score_numeric'] = pd.to_numeric(df_inspect['llm_judge_overall_quality_score'], errors='coerce')\\n\",\n",
                "                \"        \\n\",\n",
                "                \"        low_score_threshold = 0.5\\n\",\n",
                "                \"        low_score_docs = df_inspect[df_inspect['llm_judge_overall_quality_score_numeric'] < low_score_threshold]\\n\",\n",
                "                \"        \\n\",\n",
                "                '        print(f\"\\\\n--- Documents with Judge Score < {low_score_threshold} for {model_to_inspect_label} ---\")\\n',\n",
                "                \"        if not low_score_docs.empty:\\n\",\n",
                "                \"            display(low_score_docs[['celex_id', 'type_set_f1', 'llm_judge_overall_quality_score', 'llm_judge_rationale']].head())\\n\",\n",
                "                \"        else:\\n\",\n",
                "                '            print(f\"No documents found with judge score below {low_score_threshold}.\")\\n',\n",
                "                \"            \\n\",\n",
                "                \"        # Example: Find documents with a specific error (if 'error' column exists and is not None)\\n\",\n",
                "                \"        # specific_error_docs = df_inspect[df_inspect['error'].str.contains('Timeout', na=False) if 'error' in df_inspect.columns else pd.Series(False, index=df_inspect.index)]\\n\",\n",
                "                \"        # if not specific_error_docs.empty:\\n\",\n",
                "                \"        #     print(f\\\"\\\\n--- Documents with 'Timeout' error for {model_to_inspect_label} ---\\\")\\n\",\n",
                "                \"        #     display(specific_error_docs[['celex_id', 'error']].head())\\n\",\n",
                "                \"            \\n\",\n",
                "                \"    else:\\n\",\n",
                "                \"        print(f\\\"DataFrame for {model_to_inspect_label} is empty or missing 'llm_judge_overall_quality_score' column.\\\")\\n\",\n",
                "                \"else:\\n\",\n",
                "                \"    print(f\\\"Model '{model_to_inspect_label}' not found in loaded frames.\\\")\",\n",
                "            ],\n",
                "        },\n",
                "        {\n",
                "            \"cell_type\": \"markdown\",\n",
                "            \"metadata\": {},\n",
                "            \"source\": [\n",
                "                \"## 7. Conclusion\\n\",\n",
                "                \"\\n\",\n",
                "                \"This notebook provides a framework for analyzing the performance of different clause extraction models. Key takeaways include:\\n\",\n",
                "                \"*   Overall performance trends (F1, Precision, Recall, Judge Scores).\\n\",\n",
                "                \"*   The impact of changing K in RAG systems.\\n\",\n",
                "                \"*   Identification of specific documents or error types for deeper investigation.\\n\",\n",
                "                \"\\n\",\n",
                "                \"Further analysis could involve more sophisticated visualizations, statistical significance testing, or direct comparison of extracted clauses against gold standards for specific documents.\",\n",
                "            ],\n",
                "        },\n",
                "    ],\n",
                "    \"metadata\": {\n",
                "        \"kernelspec\": {\n",
                "            \"display_name\": \"Python 3 (ipykernel)\",\n",
                "            \"language\": \"python\",\n",
                "            \"name\": \"python3\",\n",
                "        },\n",
                "        \"language_info\": {\n",
                "            \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3},\n",
                "            \"file_extension\": \".py\",\n",
                "            \"mimetype\": \"text/x-python\",\n",
                "            \"name\": \"python\",\n",
                "            \"nbconvert_exporter\": \"python\",\n",
                "            \"pygments_lexer\": \"ipython3\",\n",
                "            \"version\": \"3.11.7\",\n",
                "        },\n",
                "    },\n",
                "    \"nbformat\": 4,\n",
                "    \"nbformat_minor\": 5,\n",
                "}"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}